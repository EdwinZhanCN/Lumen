# Multi-model configuration demonstrating multiple models per service
# Shows CLIP with multiple models, BioCLIP with dataset, and different runtimes

metadata:
  version: "1.0"
  region: "other"  # HuggingFace Hub
  cache_dir: "~/.lumen/models"

services:
  clip:
    enabled: true
    package: "lumen-clip"
    import:
      registry_class: "lumen_clip.service_registry.CLIPService"
      add_to_server: "lumen_clip.proto.ml_service_pb2_grpc.add_InferenceServicer_to_server"
    models:
      # Multiple models in one service
      small:
        model: "MobileCLIP2-S2"
        runtime: "onnx"
      large:
        model: "MobileCLIP-L-14"
        runtime: "torch"
      expert:
        model: "bioclip-2"
        runtime: "torch"
        dataset: "TreeOfLife-200M"  # Optional dataset for BioCLIP
    default_model: "small"  # Default to ONNX for efficiency
    env:
      CLIP_BACKEND: "auto"  # Auto-select backend
      CLIP_BATCH_SIZE: "8"
    server:
      port: 50051
      mdns:
        enabled: true
        name: "CLIP-Multi-Model-Service"
        type: "_homenative-node._tcp.local."

  face:
    enabled: true
    package: "lumen-face"
    import:
      registry_class: "lumen_face.service_registry.FaceService"
      add_to_server: "lumen_face.proto.ml_service_pb2_grpc.add_InferenceServicer_to_server"
    models:
      detection:
        model: "antelopev2"
        runtime: "onnx"
      recognition:
        model: "arcface-r100"
        runtime: "onnx"
    default_model: "detection"
    server:
      port: 50052
      mdns:
        enabled: true
        name: "Face-Service"
