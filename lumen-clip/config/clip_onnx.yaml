# 场景 5: CLIP 服务使用 ONNX Runtime
# 用途: 跨平台部署，CPU 优化推理，或使用特定硬件加速（如 OpenVINO）
# 内存占用: 低
# 性能: ONNX Runtime 通常比 PyTorch 更快（尤其在 CPU 上）

metadata:
    version: "1.0"
    region: "cn" # "other" 使用 HuggingFace, "cn" 使用 ModelScope
    cache_dir: "~/.lumen/models"

# 可选: Python 包依赖（如果需要自动安装）
dependencies:
    - "lumen-clip @ git+https://github.com/EdwinZhanCN/Lumen.git@main#subdirectory=lumen-clip"

services:
    clip:
        enabled: true
        package: "lumen-clip"
        import:
            registry_class: "image_classification.clip_service.CLIPService"
            add_to_server: "ml_service_pb2_grpc.add_InferenceServicer_to_server"
        models:
            default:
                model: "MobileCLIP2-S2"
                runtime: "onnx"
                # ONNX Runtime 会自动从以下路径加载模型:
                # ~/.lumen/models/MobileCLIP2-S2/onnx/vision.onnx
                # ~/.lumen/models/MobileCLIP2-S2/onnx/text.onnx
        default_model: "default"
        env:
            # ONNX Runtime 执行提供器（优先级从高到低）
            # 常见选项:
            # - CPUExecutionProvider (默认，所有平台)
            # - CUDAExecutionProvider (NVIDIA GPU)
            # - TensorrtExecutionProvider (NVIDIA GPU with TensorRT)
            # - CoreMLExecutionProvider (macOS/iOS)
            # - OpenVINOExecutionProvider (Intel CPU/GPU/VPU)
            # - DmlExecutionProvider (Windows DirectML)
            ONNX_PROVIDERS: "CPUExecutionProvider"
            BATCH_SIZE: "8"
        server:
            port: 50051
            mdns:
                enabled: true
                name: "CLIP-ONNX-Service"
                type: "_lumen-clip._tcp.local."
